{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Email Spam Classifier - BERT</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is based on Manning's book \"Transfer Learning for NLP\" (chapter 2).\n",
    "The goal here is:\n",
    "\n",
    "1. Curate a dataset with emails and spam consisting of random 1000 samples for each class\n",
    "2. Extract from the emails only the text,i.e, no headers.\n",
    "3. Create a simple bag-of-words model from the above content. Simple because it is based on term frequency (tf) only.\n",
    "4. Choose one baseline classifier from Logistic Regression and Gradient Boosting Machine\n",
    "5. Accuracy is the metric of choice as the dataset is balanced and consists of two classes\n",
    "6. Train a SPAM classifier based on BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requiered libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import email\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define data file path, i.e., directory where data to train the classifier is. As this is a csv type,  the file is loaded with pandas read_csv function. If successfull the number of rows and columns with the first 5 rows are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Emails were loaded successfully containing 517401 rows and 2 columns\nPrinting 5 first rows...\n                       file                                            message\n0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\n"
     ]
    }
   ],
   "source": [
    "data_file_path = \"/home/baosiek/Documents/deep_learning/transfer-learning/data/emails.csv\"\n",
    "emails = pd.read_csv(data_file_path)\n",
    "print(\"Emails were loaded successfully containing {} rows and {} columns\".format(emails.shape[0], emails.shape[1]))\n",
    "print(\"Printing 5 first rows...\")\n",
    "print(emails.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints the contect of the first email, under the message column, enabling data understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Message-ID: <18782981.1075855378110.JavaMail.evans@thyme>\nDate: Mon, 14 May 2001 16:39:00 -0700 (PDT)\nFrom: phillip.allen@enron.com\nTo: tim.belden@enron.com\nSubject: \nMime-Version: 1.0\nContent-Type: text/plain; charset=us-ascii\nContent-Transfer-Encoding: 7bit\nX-From: Phillip K Allen\nX-To: Tim Belden <Tim Belden/Enron@EnronXGate>\nX-cc: \nX-bcc: \nX-Folder: \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail\nX-Origin: Allen-P\nX-FileName: pallen (Non-Privileged).pst\n\nHere is our forecast\n\n \n"
     ]
    }
   ],
   "source": [
    "print(emails.loc[0, \"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get only the text part of the message, discarding date, from, to and Subject info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(emails):\n",
    "\n",
    "    # Initializes a list of texts where each row will contain only the content of the email\n",
    "    contents = []\n",
    "    \n",
    "    for item in emails[\"message\"]:\n",
    "        e = email.message_from_string(item)\n",
    "        content = e.get_payload() # Gets only a string with the email content\n",
    "        contents.append(content)\n",
    "        \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I tried the new address but I don't have access.  also, what do I need to \nenter under domain?\n"
     ]
    }
   ],
   "source": [
    "# This process may take sometime. So we first check if the emails were already\n",
    "# processed with their respective content in a list. If this data is not found this \n",
    "# procedure extracts the content from the emails and stores it.\n",
    "if not os.path.exists('/home/baosiek/Documents/deep_learning/transfer-learning/data/contents.txt'):\n",
    "    contents = extract_text(emails)\n",
    "    with open(\"./data/contents.txt\", \"wb\") as fp:   # Serializing\n",
    "        pickle.dump(contents, fp)\n",
    "else:\n",
    "    with open(\"./data/contents.txt\", \"rb\") as fp:   # Deserializing\n",
    "        contents = pickle.load(fp)\n",
    "        \n",
    "\n",
    "# Prints the content at row 100\n",
    "print(contents[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if number of rows in emails data frame and contents list data structure are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "if len(contents) == emails.shape[0]:\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts contents list into a data frame and prints the first 5 emails contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                   0\n0                          Here is our forecast\\n\\n \n1  Traveling to have a business meeting takes the...\n2                     test successful.  way to go!!!\n3  Randy,\\n\\n Can you send me a schedule of the s...\n4                Let's shoot for Tuesday at 11:45.  \n"
     ]
    }
   ],
   "source": [
    "contents_df = pd.DataFrame(contents)\n",
    "print(contents_df.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Spam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Spams was successfully downloaded and contains 3977 emails\n",
      "                                                   0\n",
      "0  FROM:MR. JAMES NGOLA.\\nCONFIDENTIAL TEL: 233-2...\n",
      "1  Dear Friend,\\n\\nI am Mr. Ben Suleman a custom ...\n",
      "2  FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF EL...\n",
      "3  FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF EL...\n",
      "4  Dear sir, \\n \\nIt is with a heart full of hope...\n"
     ]
    }
   ],
   "source": [
    "data_file_path = \"/home/baosiek/Documents/deep_learning/transfer-learning/data/fradulent_emails.txt\"\n",
    "with open(data_file_path, 'r', encoding='latin1') as file:\n",
    "    spams = file.read()\n",
    "# spams is a long string beacause originally the data downloaded is a single text file.\n",
    "# We need to find how to split this entire file into units where each one in a single email.\n",
    "# So reading the downloaded file with Gedit we can conclude that each email starts with\n",
    "# the char sequence \"From r\". So we will use this to split this big string into a list of emails.\n",
    "# The reason why we associate to the final structure all emails from 1 and not 0 is because the \n",
    "# original file starts with 'From r' splitting it into an empty first element and the first email in\n",
    "# the second elemnt of the spams list\n",
    "spams = spams.split('From r')[1:]\n",
    "print(f'Spams was successfully downloaded and contains {len(spams)} emails')\n",
    "\n",
    "spams_df = extract_text(pd.DataFrame(spams, columns=['message']))\n",
    "spams_df = pd.DataFrame(spams_df)\n",
    "print(spams_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create the dataset to train classifiers. It will contain n_samples from each data frame (emails and spams). Each sample will contain max_tokens and each token max_characters as hyperparameters to enable acceptable performance for training and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000 # number of samples frome each data frame\n",
    "max_tokens = 50 # maximum number of tokens in each email\n",
    "max_chars = 20 # maximum length of each token\n",
    "threshold = 0.7 # percentage of training examples in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to tokenize emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    \n",
    "    if row == None or row == '' or type(row) == list:\n",
    "        tokens = \"\"\n",
    "    else:\n",
    "        tokens = row.split(\" \")[:max_tokens]\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract punctuations, lowercase all tokens anf limit token size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(token_list):\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    try:\n",
    "        for token in token_list:\n",
    "            token = token.lower()\n",
    "            token = re.sub(r'[\\W\\d]', \"\", token)[:max_chars]\n",
    "            tokens.append(token)\n",
    "            \n",
    "    except:\n",
    "        tokens.append(\"\")\n",
    "        \n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dataframe with emails and spams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/baosiek/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def stopwords_removal(token_list):\n",
    "    \n",
    "    token = [token for token in token_list if token not in stopwords]\n",
    "    token = filter(None, token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "real_emails dataframe contains 1000 samples. Listing first 5...\n389288    [, forwarded, elizabeth, sagerhouect, , , am, ...\n468101    [, crisis, and, opportunitypower, markets, mar...\n326517    [maci, sent, directly, ahmed, via, email, than...\n293169    [the, issue, respect, addressed, one, simple, ...\n184065    [i, conv, wmark, marcus, nettleton, today, par...\nName: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "real_emails = contents_df.iloc[:, 0]\n",
    "real_emails = real_emails.apply(tokenize)\n",
    "real_emails = real_emails.apply(stopwords_removal)\n",
    "real_emails = real_emails.apply(clean_text)\n",
    "real_emails = real_emails.sample(n_samples)\n",
    "print(f'real_emails dataframe contains {real_emails.shape[0]} samples. Listing first 5...')\n",
    "print(real_emails.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spam_emails dataframe contains 1000 samples. Listing first 5...\n2423                                                   []\n345     [from, george, dukephone, , , , greetingthis, ...\n2851    [th, floorguangxing, building, th, nanshan, ro...\n1329    [from, mreric, udeno, , george, avenueoppst, s...\n551     [attention, ceo, presidentfrom, mr, davis, tut...\nName: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "spam_emails = spams_df.iloc[:, 0]\n",
    "spam_emails = spam_emails.apply(tokenize)\n",
    "spam_emails = spam_emails.apply(stopwords_removal)\n",
    "spam_emails = spam_emails.apply(clean_text)\n",
    "spam_emails = spam_emails.sample(n_samples)\n",
    "print(f'spam_emails dataframe contains {spam_emails.shape[0]} samples. Listing first 5...')\n",
    "print(spam_emails.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets combine these two data frames into one NP array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data is a <class 'numpy.ndarray'>. Shape is (2000,).\n['from', 'george', 'dukephone', '', '', '', 'greetingthis', 'letter', 'might', 'surprise', 'met', 'neither', 'person', 'nor', 'correspondence', 'but', 'i', 'believe', 'one', 'day', 'get', 'know', 'somebody', 'either', 'physical', 'correspondence', 'iswhy', 'in', 'spain', 'making', 'contart']\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([spam_emails, real_emails], axis=0).values\n",
    "print(f'data is a {type(data)}. Shape is {data.shape}.')\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creting labels to the emails in data. The first 1000 columsn [:1000] are real emails (label=1) and the last 1000 [1000:] are spams (label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "labels has shape 2000\n[1, 1, 1, 1, 1]\n[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "categories = ['spam''real']\n",
    "labels = ([1]*n_samples) # spams\n",
    "labels.extend(([0]*n_samples)) # emails\n",
    "\n",
    "print(f'labels has shape {len(labels)}')\n",
    "print(labels[:5]) # printing first 5 real labels\n",
    "print(labels[1000: 1005]) # printing first spam labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required libraries for tensorflow on CPU and keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensorflow version is 1.15.0. Correct!\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__ == \"1.15.0\":\n",
    "    print(f'Tensorflow version is {tf.__version__}. Correct!')\n",
    "else:\n",
    "    print(f'Tensorflow version is {tf.__version__}. Should be 1.15.0! Failed')\n",
    "    \n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "source": [
    "# BERT layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "        n_fine_tune_layer = 10,\n",
    "        pooling = 'mean',\n",
    "        bert_path = 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1',\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        self.n_fine_tune_layer = n_fine_tune_layer\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_features):\n",
    "\n",
    "        self.bert_layer = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f'{self.name}_module')\n",
    "\n",
    "        trainable_vars = self.bert_layer.variables\n",
    "        if self.pooling == 'first':\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "        elif self.pooling == 'mean':\n",
    "            trainable_vars = [\n",
    "                var for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name \n",
    "                and not \"/pooler/\" in var.name]\n",
    "\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError('Undefined pooling type')\n",
    "\n",
    "        for i in range(self.n_fine_tune_layer):\n",
    "            trainable_layers.append(f'encoder/layer_{str(11 - i)}')\n",
    "\n",
    "        trainable_vars = [\n",
    "            var for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "            ]\n",
    "\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_features)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype='int32') for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
    "\n",
    "        if self.pooling == 'first':\n",
    "            pooled = self.bert_layer(\n",
    "                inputs=bert_inputs, signature='tokens', as_dict=True)['pooled_output']\n",
    "\n",
    "        elif self.pooling == 'mean':\n",
    "            result = self.bert_layer(\n",
    "                inputs=bert_inputs, signature='tokens', as_dict=True)['sequence_output']\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (tf.reduce_sum(m, axis=1, keep_dims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "\n",
    "        else:\n",
    "            raise NameError('Undefined pooling type')\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)\n",
    "            \n"
   ]
  },
  {
   "source": [
    "Before preparing data to the format expected by BERT I am going to convert data format from where each email consistes in a Ã§ist of tokens to another where each email consists of one string. Starting this process from the already processed data is due to the fact that in the latter, token were alredy filtered for punctuation, stopwords, lowercase, etc. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_bert_feature_format(x, y):\n",
    "\n",
    "    converted_data, converted_labels = [], []\n",
    "    \n",
    "    # From list of tokens to one string\n",
    "    for index in range(x.shape[0]):\n",
    "        text = ' '.join(x[index])\n",
    "        converted_data.append(text)\n",
    "        converted_labels.append(y[index])\n",
    "\n",
    "    # Converted_data to np.array\n",
    "    converted_data = np.array(converted_data, dtype=object)[:, np.newaxis]\n",
    "\n",
    "    return converted_data, np.array(converted_labels)\n",
    "\n",
    "data, labels = convert_data_to_bert_feature_format(data, labels)\n",
    "\n",
    "print(f'{data[1]} -> {labels[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(data.shape[0]*threshold)\n",
    "train_x, train_y = data[:idx], labels[:idx]\n",
    "test_x, test_y = data[idx:], labels[idx:]\n",
    "\n",
    "print(f'train_x shape: {train_x.shape}, train_y shape: {train_y.shape}')\n",
    "print(f'test_x shape: {test_x.shape}, test_y shape: {test_y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_seq_length):\n",
    "\n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name='input_ids')\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name='input_masks')\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name='segment_ids')\n",
    "    bert_layer_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "    bert_output = BertLayer(n_fine_tune_layer=0)(bert_layer_inputs)\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=bert_layer_inputs, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "source": [
    "At the time of developing this notebook bert-tensorflow version 1.0.4 had an opened issue (https://github.com/google-research/bert/issues/1133). I had to downgrade it, as guided in the reffered issue, to version 1.0.1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import kerasBert as kb\n",
    "import bert.tokenization as tk\n",
    "from tensorflow_hub import Module\n",
    "import pkg_resources\n",
    "import bert_utils as bu\n",
    "\n",
    "if pkg_resources.get_distribution(\"bert-tensorflow\").version != '1.0.1':\n",
    "    raise NameError(f'bert-tensorflow version: {pkg_resources.get_distribution(\"bert-tensorflow\").version} is wrong. Correct is version 1.0.1')\n",
    "else:\n",
    "    print(f'bert-tensorflow version: {pkg_resources.get_distribution(\"bert-tensorflow\").version}')\n",
    "\n",
    "vocab_file_path = '/home/baosiek/Documents/deep_learning/transfer-learning/model/bert/assets/vocab.txt'\n",
    "tokenizer = bu.create_tokenizer(vocab_file_path)\n",
    "\n",
    "train_examples = kb.convert_text_to_examples(train_x, train_y)\n",
    "test_examples = kb.convert_text_to_examples(test_x, test_y)\n",
    "\n",
    "# (train_input_ids, train_input_masks, train_segment_ids, train_labels) = kb.convert_examples_to_features(\n",
    "#     tokenizer, train_examples, max_seq_length=max_tokens)\n",
    "\n",
    "# (test_input_ids, test_input_masks, test_segment_ids, test_labels) = convert_examples_to_features(\n",
    "#     tokenizer, test_examples, max_seq_length=max_tokens)\n",
    "\n",
    "# model = build_model(max_tokens)\n",
    "\n",
    "# initialize_vars(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('model/elmo_model.h5'):\n",
    "    print('Build and training model!')\n",
    "    model = build_model()\n",
    "    history = model.fit(train_X, train_y, validation_data=(test_X, test_y), epochs=5, batch_size=32)\n",
    "    keras.models.save_model(model, 'model/elmo_model.h5')\n",
    "    with open('model/train_history_dict', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "else:\n",
    "    print('Loading trained model!')\n",
    "    model = keras.models.load_model('model/elmo_model.h5', {'ElmoEmbeddingLayer': ElmoEmbeddingLayer})\n",
    "    with open(\"model/train_history_dict\", \"rb\") as fp:\n",
    "        history = pickle.load(fp)\n",
    "\n",
    "# ploting history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# ploting history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "toPredict = [['This is an email'],['The Royal family lives in Buckingham Palace']]\n",
    "for i in range(len(toPredict)):\n",
    "    predict = model.predict([toPredict[i]])\n",
    "    if predict[0] < 0.5:\n",
    "        print(f'{toPredict[i]} is an EMAIL!')\n",
    "    else:\n",
    "        print(f'{toPredict[i]} is SPAM!')"
   ]
  },
  {
   "source": [
    "In my experience the above results are uncommon. Epoch 5/5 had a training accuracy of 0.9864 lower than validation accuracy of 0.9883. Base model had an accuracy of 0.9550. It seems that the model is neither under or overfitting. One should remember the hyperparameters of max_chars (the maximum number of characters in the emails) and max_tokens (maximum number of tokens per email) can be changed to reaching even better performance."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tl': conda)",
   "language": "python",
   "name": "python37964bittlcondac3b95453ae5d46cea17442527270ea62"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}